# -*- coding: utf-8 -*-
"""PPM - Keep it Dry

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c9h-5K8UQCkU7aPojBGTNXoHwaWtKd3o

#Preprocess
"""

import pandas as pd
import numpy as np

#Membaca dataset ini download dari kaggle yah
df_train = pd.read_csv('train.csv')
df_test = pd.read_csv('test.csv')

df_train.head()

df_train.describe(include=['O'])

df_train.describe()

#menghapus kolom name dan passengerId karena menurut aku dua fitur itu gangaruh ke modelling nanti
columns_to_drop = ['id']
df_train.drop(columns=columns_to_drop, inplace=True)
df_test.drop(columns=columns_to_drop, inplace=True)

"""##Numerical data"""

df_train.isnull().sum()

df_test.isnull().sum()

columns_to_fill = ['loading', 'measurement_0', 'measurement_1', 'measurement_2', 'measurement_3', 'measurement_4', 'measurement_5', 'measurement_6', 'measurement_7', 'measurement_8', 'measurement_9', 'measurement_10', 'measurement_11', 'measurement_12', 'measurement_13', 'measurement_14', 'measurement_15', 'measurement_16', 'measurement_17']
for column in columns_to_fill:
    mean_value = df_train[column].mean()
    df_train[column].fillna(mean_value, inplace=True)

columns_to_fill = ['loading', 'measurement_0', 'measurement_1', 'measurement_2', 'measurement_3', 'measurement_4', 'measurement_5', 'measurement_6', 'measurement_7', 'measurement_8', 'measurement_9', 'measurement_10', 'measurement_11', 'measurement_12', 'measurement_13', 'measurement_14', 'measurement_15', 'measurement_16', 'measurement_17']
for column in columns_to_fill:
    mean_value = df_test[column].mean()
    df_test[column].fillna(mean_value, inplace=True)

df_train.isnull().sum()

df_test.isnull().sum()

"""#Categorical data"""

df_train['product_code'].unique()

df_test['product_code'].unique()

df_train['attribute_0'].unique()

df_test['attribute_0'].unique()

df_train['attribute_1'].unique()

df_test['attribute_1'].unique()

mapping = {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8}

df_train['product_code'] = df_train['product_code'].map(mapping)
df_test['product_code'] = df_test['product_code'].map(mapping)

material_mapping = {'material_5': 0, 'material_6': 1, 'material_7': 2, 'material_8': 3}

df_train['attribute_0'] = df_train['attribute_0'].map(material_mapping)
df_test['attribute_0'] = df_test['attribute_0'].map(material_mapping)

df_train['attribute_1'] = df_train['attribute_1'].map(material_mapping)
df_test['attribute_1'] = df_test['attribute_1'].map(material_mapping)

df_train

df_test

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

scaled = scaler.fit_transform(df_train)
df_train_scaled = pd.DataFrame(scaled, columns=df_train.columns)
df_train_scaled.head()

scaled = scaler.fit_transform(df_test)
df_test_scaled = pd.DataFrame(scaled, columns=df_test.columns)
df_test_scaled.head()

"""#Modelling"""

from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split

X_train = df_train_scaled.drop(['failure'], axis=1)
y_train = df_train_scaled['failure']
X_test = df_test_scaled.copy()
X_train.shape, y_train.shape, X_test.shape

df_test

df_train

svc = SVC()
svc.fit(X_train, y_train)
Y_pred = svc.predict(X_test)
score_svc = svc.score(X_train, y_train)
score_svc

knn = KNeighborsClassifier(n_neighbors = 3)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)
score_knn = knn.score(X_train, y_train)
score_knn

gaussian = GaussianNB()
gaussian.fit(X_train, y_train)
nb_pred = gaussian.predict(X_test)
score_gaussian = gaussian.score(X_train, y_train)
score_gaussian

random_forest = RandomForestClassifier()
random_forest.fit(X_train, y_train)
rf_pred = random_forest.predict(X_test)
random_forest.score(X_train, y_train)
score_rf = random_forest.score(X_train, y_train)
score_rf

ada_boost = AdaBoostClassifier()
ada_boost.fit(X_train, y_train)
ada_pred = ada_boost.predict(X_test)
ada_boost.score(X_train, y_train)
score_ab = ada_boost.score(X_train, y_train)
score_ab

# gb = GradientBoostingClassifier()

# param_grid = {'loss' : ["deviance"],
#               'n_estimators' : [100, 200, 300, 400],
#               'learning_rate': [0.1, 0.05, 0.01, 0.001],
#               'max_depth': [4, 8],
#               'min_samples_leaf': [100, 150],
#               'max_features': [0.3, 0.2, 0.1]
#               }

# grid_search = GridSearchCV(estimator=gb, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)

# grid_search.fit(X_train, y_train)

# best_params = grid_search.best_params_
# print("Best Hyperparameters:", best_params)

# gb.fit(X_train,y_train)
# gb_pred = gb.predict(X_test)
# print(gb.score(X_train, y_train))

gradient_boost = GradientBoostingClassifier()
gradient_boost.fit(X_train, y_train)
gb_pred = gradient_boost.predict(X_test)
print(gradient_boost.score(X_train, y_train))

decision_tree = DecisionTreeClassifier()
decision_tree.fit(X_train, y_train)
dt_pred = decision_tree.predict(X_test)
print(decision_tree.score(X_train, y_train))

"""#Submit to Kaggle"""

submission_sample = pd.read_csv('sample_submission.csv')

submission_sample.head()

df_final = pd.DataFrame({ "id": submission_sample["id"],"failure": gb_pred})
df_final

df_final.to_csv('prediction_result_gb_mean.csv', index=False)